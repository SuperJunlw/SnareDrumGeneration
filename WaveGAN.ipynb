{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y339IytrBbSb"
      },
      "outputs": [],
      "source": [
        "!pip3 install librosa pescador matplotlib numpy torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCF9fcjEUx3m"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# def copy_wav_files(source_folder, destination_folder):\n",
        "#     # Ensure the destination folder exists\n",
        "#     os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "#     # Iterate over all files in the source folder\n",
        "#     for filename in os.listdir(source_folder):\n",
        "#         # Check if the file has a .wav extension\n",
        "#         if filename.lower().endswith('.wav'):\n",
        "#             source_file = os.path.join(source_folder, filename)\n",
        "#             # Check if it's a file and not a folder\n",
        "#             if os.path.isfile(source_file):\n",
        "#                 destination_file = os.path.join(destination_folder, filename)\n",
        "#                 shutil.copy2(source_file, destination_file)  # copy2 preserves metadata\n",
        "#                 print(f\"Copied {filename} to {destination_folder}\")\n",
        "\n",
        "# # Example usage\n",
        "# source_folder = '/content/drive/MyDrive/snare_samples/16bit_mono_wavs_clipped/validation'\n",
        "# destination_folder = '/content/drive/MyDrive/ECS271/snare_sample_valid_wavOnly'\n",
        "# copy_wav_files(source_folder, destination_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBxNiWsRCi0t",
        "outputId": "0a777de1-78ca-408e-dc2d-d533981d614e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD5jvY1wMYaw"
      },
      "source": [
        "# Parameter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAxgL-QFFXVP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "# Paths\n",
        "target_signals_dir = '/content/drive/MyDrive/ECS271/snare_samples/16bit_mono_wavs_clipped_22050'  # Update this path\n",
        "output_dir = '/content/drive/MyDrive/ECS271'  # Update this path\n",
        "\n",
        "# Model Parameters\n",
        "model_prefix = 'exp1'\n",
        "n_iterations = 160000 #100000, 1000, 10000, 20000\n",
        "use_batchnorm = False\n",
        "lr_g = 1e-5\n",
        "lr_d = 1e-5\n",
        "beta1 = 0.5\n",
        "beta2 = 0.9\n",
        "decay_lr = False\n",
        "generator_batch_size_factor = 1\n",
        "n_critic = 1\n",
        "validate = False\n",
        "p_coeff = 10\n",
        "batch_size = 30\n",
        "noise_latent_dim = 100\n",
        "model_capacity_size = 32\n",
        "store_cost_every = 300 #300\n",
        "progress_bar_step_iter_size = 400\n",
        "window_length = 32768 #32768 65536\n",
        "sampling_rate = 22050 #22050 16000\n",
        "normalize_audio = True\n",
        "num_channels = 1\n",
        "\n",
        "take_backup = True\n",
        "backup_every_n_iters = 1000\n",
        "save_samples_every = 1000\n",
        "output_dir = 'output'\n",
        "if not(os.path.isdir(output_dir)):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "#############################\n",
        "# Logger init\n",
        "#############################\n",
        "LOGGER = logging.getLogger('wavegan')\n",
        "LOGGER.setLevel(logging.DEBUG)\n",
        "#############################\n",
        "# Torch Init and seed setting\n",
        "#############################\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "# update the seed\n",
        "manual_seed = 826\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "np.random.seed(manual_seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(manual_seed)\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fewMwBpwF62B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import pescador\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL4nw-4oMedG"
      },
      "source": [
        "# Utili\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czto5-TaGNge"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "def get_recursive_files(folderPath, ext):\n",
        "    results = os.listdir(folderPath)\n",
        "    outFiles = []\n",
        "    for file in results:\n",
        "        if os.path.isdir(os.path.join(folderPath, file)):\n",
        "            outFiles += get_recursive_files(os.path.join(folderPath, file), ext)\n",
        "        elif file.endswith(ext):\n",
        "            outFiles.append(os.path.join(folderPath, file))\n",
        "\n",
        "    return outFiles\n",
        "\n",
        "\n",
        "def make_path(output_path):\n",
        "    if not os.path.isdir(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    return output_path\n",
        "\n",
        "\n",
        "#############################\n",
        "# Plotting utils\n",
        "#############################\n",
        "def visualize_audio(audio_tensor, is_monphonic=False):\n",
        "    # takes a batch ,n channels , window length and plots the spectogram\n",
        "    input_audios = audio_tensor.detach().cpu().numpy()\n",
        "    plt.figure(figsize=(18, 50))\n",
        "    for i, audio in enumerate(input_audios):\n",
        "        plt.subplot(10, 2, i + 1)\n",
        "        if is_monphonic:\n",
        "            plt.title(\"Monophonic %i\" % (i + 1))\n",
        "            librosa.display.waveshow(audio[0], sr=sampling_rate)\n",
        "        else:\n",
        "            D = librosa.amplitude_to_db(np.abs(librosa.stft(audio[0])), ref=np.max)\n",
        "            librosa.display.specshow(D, y_axis=\"linear\")\n",
        "            plt.colorbar(format=\"%+2.0f dB\")\n",
        "            plt.title(\"Linear-frequency power spectrogram %i\" % (i + 1))\n",
        "    if not (os.path.isdir(\"visualization\")):\n",
        "        os.makedirs(\"visualization\")\n",
        "    plt.savefig(\"visualization/interpolation.png\")\n",
        "\n",
        "\n",
        "def visualize_loss(loss_1, loss_2, first_legend, second_legend, y_label):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"{} and {} Loss During Training\".format(first_legend, second_legend))\n",
        "    plt.plot(loss_1, label=first_legend)\n",
        "    plt.plot(loss_2, label=second_legend)\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(y_label)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    if not (os.path.isdir(\"visualization\")):\n",
        "        os.makedirs(\"visualization\")\n",
        "    plt.savefig(\"visualization/loss.png\")\n",
        "\n",
        "\n",
        "def latent_space_interpolation(model, n_samples=10):\n",
        "    z_test = sample_noise(2)\n",
        "    with torch.no_grad():\n",
        "        interpolates = []\n",
        "        for alpha in np.linspace(0, 1, n_samples):\n",
        "            interpolate_vec = alpha * z_test[0] + ((1 - alpha) * z_test[1])\n",
        "            interpolates.append(interpolate_vec)\n",
        "\n",
        "        interpolates = torch.stack(interpolates)\n",
        "        generated_audio = model(interpolates)\n",
        "    visualize_audio(generated_audio, True)\n",
        "\n",
        "\n",
        "#############################\n",
        "# Wav files utils\n",
        "#############################\n",
        "# Fast loading used with wav files only of 8 bits\n",
        "def load_wav(wav_file_path):\n",
        "    try:\n",
        "        audio_data, _ = librosa.load(wav_file_path, sr=sampling_rate)\n",
        "\n",
        "        if normalize_audio:\n",
        "            # Clip magnitude\n",
        "            max_mag = np.max(np.abs(audio_data))\n",
        "            if max_mag > 1:\n",
        "                audio_data /= max_mag\n",
        "    except Exception as e:\n",
        "        LOGGER.error(\"Could not load {}: {}\".format(wav_file_path, str(e)))\n",
        "        raise e\n",
        "    audio_len = len(audio_data)\n",
        "    if audio_len < window_length:\n",
        "        pad_length = window_length - audio_len\n",
        "        left_pad = pad_length // 2\n",
        "        right_pad = pad_length - left_pad\n",
        "        audio_data = np.pad(audio_data, (left_pad, right_pad), mode=\"constant\")\n",
        "\n",
        "    return audio_data.astype(\"float32\")\n",
        "\n",
        "\n",
        "def sample_audio(audio_data, start_idx=None, end_idx=None):\n",
        "    audio_len = len(audio_data)\n",
        "    if audio_len == window_length:\n",
        "        # If we only have a single 1*window_length audio, just yield.\n",
        "        sample = audio_data\n",
        "    else:\n",
        "        # Sample a random window from the audio\n",
        "        if start_idx is None or end_idx is None:\n",
        "            start_idx = np.random.randint(0, (audio_len - window_length) // 2)\n",
        "            end_idx = start_idx + window_length\n",
        "        sample = audio_data[start_idx:end_idx]\n",
        "    sample = sample.astype(\"float32\")\n",
        "    assert not np.any(np.isnan(sample))\n",
        "    return sample, start_idx, end_idx\n",
        "\n",
        "\n",
        "def sample_buffer(buffer_data, start_idx=None, end_idx=None):\n",
        "    audio_len = len(buffer_data) // 4\n",
        "    if audio_len == window_length:\n",
        "        # If we only have a single 1*window_length audio, just yield.\n",
        "        sample = buffer_data\n",
        "    else:\n",
        "        # Sample a random window from the audio\n",
        "        if start_idx is None or end_idx is None:\n",
        "            start_idx = np.random.randint(0, (audio_len - window_length) // 2)\n",
        "            end_idx = start_idx + window_length\n",
        "        sample = buffer_data[start_idx * 4 : end_idx * 4]\n",
        "    return sample, start_idx, end_idx\n",
        "\n",
        "\n",
        "def wav_generator(file_path):\n",
        "    audio_data = load_wav(file_path)\n",
        "    while True:\n",
        "        sample, _, _ = sample_audio(audio_data)\n",
        "        yield {\"single\": sample}\n",
        "\n",
        "\n",
        "def create_stream_reader(single_signal_file_list):\n",
        "    data_streams = []\n",
        "    for audio_path in single_signal_file_list:\n",
        "        stream = pescador.Streamer(wav_generator, audio_path)\n",
        "        data_streams.append(stream)\n",
        "    mux = pescador.ShuffledMux(data_streams)\n",
        "    batch_gen = pescador.buffer_stream(mux, batch_size)\n",
        "    return batch_gen\n",
        "\n",
        "\n",
        "def save_samples(epoch_samples, epoch):\n",
        "    \"\"\"\n",
        "    Save output samples.\n",
        "    \"\"\"\n",
        "    sample_dir = make_path(os.path.join(output_dir, str(epoch)))\n",
        "\n",
        "    for idx, sample in enumerate(epoch_samples):\n",
        "        output_path = os.path.join(sample_dir, \"{}.wav\".format(idx + 1))\n",
        "        sample = sample[0]\n",
        "        sf.write(output_path, sample, sampling_rate)\n",
        "\n",
        "\n",
        "#############################\n",
        "# Sampling from model\n",
        "#############################\n",
        "def sample_noise(size):\n",
        "    z = torch.FloatTensor(size, noise_latent_dim).to(device)\n",
        "    z.data.normal_()  # generating latent space based on normal distribution\n",
        "    return z\n",
        "\n",
        "\n",
        "#############################\n",
        "# Model Utils\n",
        "#############################\n",
        "\n",
        "\n",
        "def update_optimizer_lr(optimizer, lr, decay):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr * decay\n",
        "\n",
        "\n",
        "def gradients_status(model, flag):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = flag\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv1d):\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "#############################\n",
        "# Creating Data Loader and Sampler\n",
        "#############################\n",
        "class WavDataLoader:\n",
        "    def __init__(self, folder_path, audio_extension=\"wav\"):\n",
        "        self.signal_paths = get_recursive_files(folder_path, audio_extension)\n",
        "        self.data_iter = None\n",
        "        self.initialize_iterator()\n",
        "\n",
        "    def initialize_iterator(self):\n",
        "        data_iter = create_stream_reader(self.signal_paths)\n",
        "        self.data_iter = iter(data_iter)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.signal_paths)\n",
        "\n",
        "    def numpy_to_tensor(self, numpy_array):\n",
        "        numpy_array = numpy_array[:, np.newaxis, :]\n",
        "        return torch.Tensor(numpy_array).to(device)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        x = next(self.data_iter)\n",
        "        return self.numpy_to_tensor(x[\"single\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ62PT7mMn7P"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuW0IzkiGPB2"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Transpose1dLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride,\n",
        "        padding=11,\n",
        "        upsample=None,\n",
        "        output_padding=1,\n",
        "        use_batch_norm=False,\n",
        "    ):\n",
        "        super(Transpose1dLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_pad = nn.ConstantPad1d(kernel_size // 2, value=0)\n",
        "        conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride)\n",
        "        conv1d.weight.data.normal_(0.0, 0.02)\n",
        "        Conv1dTrans = nn.ConvTranspose1d(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, output_padding\n",
        "        )\n",
        "        batch_norm = nn.BatchNorm1d(out_channels)\n",
        "        if self.upsample:\n",
        "            operation_list = [reflection_pad, conv1d]\n",
        "        else:\n",
        "            operation_list = [Conv1dTrans]\n",
        "\n",
        "        if use_batch_norm:\n",
        "            operation_list.append(batch_norm)\n",
        "        self.transpose_ops = nn.Sequential(*operation_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            # recommended by wavgan paper to use nearest upsampling\n",
        "            x = nn.functional.interpolate(x, scale_factor=self.upsample, mode=\"nearest\")\n",
        "        return self.transpose_ops(x)\n",
        "\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels,\n",
        "        output_channels,\n",
        "        kernel_size,\n",
        "        alpha=0.2,\n",
        "        shift_factor=2,\n",
        "        stride=4,\n",
        "        padding=11,\n",
        "        use_batch_norm=False,\n",
        "        drop_prob=0,\n",
        "    ):\n",
        "        super(Conv1D, self).__init__()\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            input_channels, output_channels, kernel_size, stride=stride, padding=padding\n",
        "        )\n",
        "        self.batch_norm = nn.BatchNorm1d(output_channels)\n",
        "        self.phase_shuffle = PhaseShuffle(shift_factor)\n",
        "        self.alpha = alpha\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_phase_shuffle = shift_factor == 0\n",
        "        self.use_drop = drop_prob > 0\n",
        "        self.dropout = nn.Dropout2d(drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1d(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm(x)\n",
        "        x = F.leaky_relu(x, negative_slope=self.alpha)\n",
        "        if self.use_phase_shuffle:\n",
        "            x = self.phase_shuffle(x)\n",
        "        if self.use_drop:\n",
        "            x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PhaseShuffle(nn.Module):\n",
        "    \"\"\"\n",
        "    Performs phase shuffling, i.e. shifting feature axis of a 3D tensor\n",
        "    by a random integer in {-n, n} and performing reflection padding where\n",
        "    necessary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Copied from https://github.com/jtcramer/wavegan/blob/master/wavegan.py#L8\n",
        "    def __init__(self, shift_factor):\n",
        "        super(PhaseShuffle, self).__init__()\n",
        "        self.shift_factor = shift_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.shift_factor == 0:\n",
        "            return x\n",
        "        # uniform in (L, R)\n",
        "        k_list = (\n",
        "            torch.Tensor(x.shape[0]).random_(0, 2 * self.shift_factor + 1)\n",
        "            - self.shift_factor\n",
        "        )\n",
        "        k_list = k_list.numpy().astype(int)\n",
        "\n",
        "        # Combine sample indices into lists so that less shuffle operations\n",
        "        # need to be performed\n",
        "        k_map = {}\n",
        "        for idx, k in enumerate(k_list):\n",
        "            k = int(k)\n",
        "            if k not in k_map:\n",
        "                k_map[k] = []\n",
        "            k_map[k].append(idx)\n",
        "\n",
        "        # Make a copy of x for our output\n",
        "        x_shuffle = x.clone()\n",
        "\n",
        "        # Apply shuffle to each sample\n",
        "        for k, idxs in k_map.items():\n",
        "            if k > 0:\n",
        "                x_shuffle[idxs] = F.pad(x[idxs][..., :-k], (k, 0), mode=\"reflect\")\n",
        "            else:\n",
        "                x_shuffle[idxs] = F.pad(x[idxs][..., -k:], (0, -k), mode=\"reflect\")\n",
        "\n",
        "        assert x_shuffle.shape == x.shape, \"{}, {}\".format(x_shuffle.shape, x.shape)\n",
        "        return x_shuffle\n",
        "\n",
        "class WaveGANGenerator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_size=64,\n",
        "        ngpus=1,\n",
        "        num_channels=1,\n",
        "        verbose=False,\n",
        "        upsample=True,\n",
        "        slice_len=16384,\n",
        "        use_batch_norm=False,\n",
        "    ):\n",
        "        super(WaveGANGenerator, self).__init__()\n",
        "        assert slice_len in [16384, 32768, 65536]  # used to predict longer utterances\n",
        "\n",
        "        self.ngpus = ngpus\n",
        "        self.model_size = model_size  # d\n",
        "        self.num_channels = num_channels  # c\n",
        "        latent_dim = noise_latent_dim\n",
        "        self.verbose = verbose\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        self.dim_mul = 16 if slice_len == 16384 else 32\n",
        "\n",
        "        self.fc1 = nn.Linear(latent_dim, 4 * 4 * model_size * self.dim_mul)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=model_size * self.dim_mul)\n",
        "\n",
        "        stride = 4\n",
        "        if upsample:\n",
        "            stride = 1\n",
        "            upsample = 4\n",
        "\n",
        "        deconv_layers = [\n",
        "            Transpose1dLayer(\n",
        "                self.dim_mul * model_size,\n",
        "                (self.dim_mul * model_size) // 2,\n",
        "                25,\n",
        "                stride,\n",
        "                upsample=upsample,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "            ),\n",
        "            Transpose1dLayer(\n",
        "                (self.dim_mul * model_size) // 2,\n",
        "                (self.dim_mul * model_size) // 4,\n",
        "                25,\n",
        "                stride,\n",
        "                upsample=upsample,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "            ),\n",
        "            Transpose1dLayer(\n",
        "                (self.dim_mul * model_size) // 4,\n",
        "                (self.dim_mul * model_size) // 8,\n",
        "                25,\n",
        "                stride,\n",
        "                upsample=upsample,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "            ),\n",
        "            Transpose1dLayer(\n",
        "                (self.dim_mul * model_size) // 8,\n",
        "                (self.dim_mul * model_size) // 16,\n",
        "                25,\n",
        "                stride,\n",
        "                upsample=upsample,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        if slice_len == 16384:\n",
        "            deconv_layers.append(\n",
        "                Transpose1dLayer(\n",
        "                    (self.dim_mul * model_size) // 16,\n",
        "                    num_channels,\n",
        "                    25,\n",
        "                    stride,\n",
        "                    upsample=upsample,\n",
        "                )\n",
        "            )\n",
        "        elif slice_len == 32768:\n",
        "            deconv_layers += [\n",
        "                Transpose1dLayer(\n",
        "                    (self.dim_mul * model_size) // 16,\n",
        "                    model_size,\n",
        "                    25,\n",
        "                    stride,\n",
        "                    upsample=upsample,\n",
        "                    use_batch_norm=use_batch_norm,\n",
        "                ),\n",
        "                Transpose1dLayer(model_size, num_channels, 25, 2, upsample=upsample),\n",
        "            ]\n",
        "        elif slice_len == 65536:\n",
        "            deconv_layers += [\n",
        "                Transpose1dLayer(\n",
        "                    (self.dim_mul * model_size) // 16,\n",
        "                    model_size,\n",
        "                    25,\n",
        "                    stride,\n",
        "                    upsample=upsample,\n",
        "                    use_batch_norm=use_batch_norm,\n",
        "                ),\n",
        "                Transpose1dLayer(\n",
        "                    model_size, num_channels, 25, stride, upsample=upsample\n",
        "                ),\n",
        "            ]\n",
        "        else:\n",
        "            raise ValueError(\"slice_len {} value is not supported\".format(slice_len))\n",
        "\n",
        "        self.deconv_list = nn.ModuleList(deconv_layers)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.ConvTranspose1d) or isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x).view(-1, self.dim_mul * self.model_size, 16)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        if self.verbose:\n",
        "            print(x.shape)\n",
        "\n",
        "        for deconv in self.deconv_list[:-1]:\n",
        "            x = F.relu(deconv(x))\n",
        "            if self.verbose:\n",
        "                print(x.shape)\n",
        "        output = torch.tanh(self.deconv_list[-1](x))\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class WaveGANDiscriminator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_size=64,\n",
        "        ngpus=1,\n",
        "        num_channels=1,\n",
        "        shift_factor=2,\n",
        "        alpha=0.2,\n",
        "        verbose=False,\n",
        "        slice_len=16384,\n",
        "        use_batch_norm=False,\n",
        "    ):\n",
        "        super(WaveGANDiscriminator, self).__init__()\n",
        "        assert slice_len in [16384, 32768, 65536]  # used to predict longer utterances\n",
        "\n",
        "        self.model_size = model_size  # d\n",
        "        self.ngpus = ngpus\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.num_channels = num_channels  # c\n",
        "        self.shift_factor = shift_factor  # n\n",
        "        self.alpha = alpha\n",
        "        self.verbose = verbose\n",
        "\n",
        "        conv_layers = [\n",
        "            Conv1D(\n",
        "                num_channels,\n",
        "                model_size,\n",
        "                25,\n",
        "                stride=4,\n",
        "                padding=11,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "                alpha=alpha,\n",
        "                shift_factor=shift_factor,\n",
        "            ),\n",
        "            Conv1D(\n",
        "                model_size,\n",
        "                2 * model_size,\n",
        "                25,\n",
        "                stride=4,\n",
        "                padding=11,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "                alpha=alpha,\n",
        "                shift_factor=shift_factor,\n",
        "            ),\n",
        "            Conv1D(\n",
        "                2 * model_size,\n",
        "                4 * model_size,\n",
        "                25,\n",
        "                stride=4,\n",
        "                padding=11,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "                alpha=alpha,\n",
        "                shift_factor=shift_factor,\n",
        "            ),\n",
        "            Conv1D(\n",
        "                4 * model_size,\n",
        "                8 * model_size,\n",
        "                25,\n",
        "                stride=4,\n",
        "                padding=11,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "                alpha=alpha,\n",
        "                shift_factor=shift_factor,\n",
        "            ),\n",
        "            Conv1D(\n",
        "                8 * model_size,\n",
        "                16 * model_size,\n",
        "                25,\n",
        "                stride=4,\n",
        "                padding=11,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "                alpha=alpha,\n",
        "                shift_factor=0 if slice_len == 16384 else shift_factor,\n",
        "            ),\n",
        "        ]\n",
        "        self.fc_input_size = 256 * model_size\n",
        "        if slice_len == 32768:\n",
        "            conv_layers.append(\n",
        "                Conv1D(\n",
        "                    16 * model_size,\n",
        "                    32 * model_size,\n",
        "                    25,\n",
        "                    stride=2,\n",
        "                    padding=11,\n",
        "                    use_batch_norm=use_batch_norm,\n",
        "                    alpha=alpha,\n",
        "                    shift_factor=0,\n",
        "                )\n",
        "            )\n",
        "            self.fc_input_size = 480 * model_size\n",
        "        elif slice_len == 65536:\n",
        "            conv_layers.append(\n",
        "                Conv1D(\n",
        "                    16 * model_size,\n",
        "                    32 * model_size,\n",
        "                    25,\n",
        "                    stride=4,\n",
        "                    padding=11,\n",
        "                    use_batch_norm=use_batch_norm,\n",
        "                    alpha=alpha,\n",
        "                    shift_factor=0,\n",
        "                )\n",
        "            )\n",
        "            self.fc_input_size = 512 * model_size\n",
        "\n",
        "        self.conv_layers = nn.ModuleList(conv_layers)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.fc_input_size, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for conv in self.conv_layers:\n",
        "            x = conv(x)\n",
        "            if self.verbose:\n",
        "                print(x.shape)\n",
        "        x = x.view(-1, self.fc_input_size)\n",
        "        if self.verbose:\n",
        "            print(x.shape)\n",
        "\n",
        "        return self.fc1(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTG6ei21Mq_7"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IOqUpQ4IFH0"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import grad, Variable\n",
        "\n",
        "class WaveGan_GP(object):\n",
        "    def __init__(self, train_loader, val_loader):\n",
        "        super(WaveGan_GP, self).__init__()\n",
        "        self.g_cost = []\n",
        "        self.train_d_cost = []\n",
        "        self.train_w_distance = []\n",
        "        self.valid_g_cost = [-1]\n",
        "        self.valid_reconstruction = []\n",
        "\n",
        "        self.discriminator = WaveGANDiscriminator(\n",
        "            slice_len=window_length,\n",
        "            model_size=model_capacity_size,\n",
        "            use_batch_norm=use_batchnorm,\n",
        "            num_channels=num_channels,\n",
        "        ).to(device)\n",
        "        self.discriminator.apply(weights_init)\n",
        "\n",
        "        self.generator = WaveGANGenerator(\n",
        "            slice_len=window_length,\n",
        "            model_size=model_capacity_size,\n",
        "            use_batch_norm=use_batchnorm,\n",
        "            num_channels=num_channels,\n",
        "        ).to(device)\n",
        "        self.generator.apply(weights_init)\n",
        "\n",
        "        self.optimizer_g = optim.Adam(\n",
        "            self.generator.parameters(), lr=lr_g, betas=(beta1, beta2)\n",
        "        )  # Setup Adam optimizers for both G and D\n",
        "        self.optimizer_d = optim.Adam(\n",
        "            self.discriminator.parameters(), lr=lr_d, betas=(beta1, beta2)\n",
        "        )\n",
        "\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "        self.validate = validate\n",
        "        self.n_samples_per_batch = len(train_loader)\n",
        "\n",
        "    def calculate_discriminator_loss(self, real, generated):\n",
        "        disc_out_gen = self.discriminator(generated)\n",
        "        disc_out_real = self.discriminator(real)\n",
        "\n",
        "        alpha = torch.FloatTensor(batch_size, 1, 1).uniform_(0, 1).to(device)\n",
        "        alpha = alpha.expand(batch_size, real.size(1), real.size(2))\n",
        "\n",
        "        interpolated = (1 - alpha) * real.data + (alpha) * generated.data[:batch_size]\n",
        "        interpolated = Variable(interpolated, requires_grad=True)\n",
        "\n",
        "        # calculate probability of interpolated examples\n",
        "        prob_interpolated = self.discriminator(interpolated)\n",
        "        grad_inputs = interpolated\n",
        "        ones = torch.ones(prob_interpolated.size()).to(device)\n",
        "        gradients = grad(\n",
        "            outputs=prob_interpolated,\n",
        "            inputs=grad_inputs,\n",
        "            grad_outputs=ones,\n",
        "            create_graph=True,\n",
        "            retain_graph=True,\n",
        "            only_inputs=True,\n",
        "        )[0]\n",
        "        # calculate gradient penalty\n",
        "        grad_penalty = (\n",
        "            p_coeff\n",
        "            * ((gradients.view(gradients.size(0), -1).norm(2, dim=1) - 1) ** 2).mean()\n",
        "        )\n",
        "        assert not (torch.isnan(grad_penalty))\n",
        "        assert not (torch.isnan(disc_out_gen.mean()))\n",
        "        assert not (torch.isnan(disc_out_real.mean()))\n",
        "        cost_wd = disc_out_gen.mean() - disc_out_real.mean()\n",
        "        cost = cost_wd + grad_penalty\n",
        "        return cost, cost_wd\n",
        "\n",
        "    def apply_zero_grad(self):\n",
        "        self.generator.zero_grad()\n",
        "        self.optimizer_g.zero_grad()\n",
        "\n",
        "        self.discriminator.zero_grad()\n",
        "        self.optimizer_d.zero_grad()\n",
        "\n",
        "    def enable_disc_disable_gen(self):\n",
        "        gradients_status(self.discriminator, True)\n",
        "        gradients_status(self.generator, False)\n",
        "\n",
        "    def enable_gen_disable_disc(self):\n",
        "        gradients_status(self.discriminator, False)\n",
        "        gradients_status(self.generator, True)\n",
        "\n",
        "    def disable_all(self):\n",
        "        gradients_status(self.discriminator, False)\n",
        "        gradients_status(self.generator, False)\n",
        "\n",
        "    def train(self):\n",
        "        progress_bar = tqdm(total=n_iterations // progress_bar_step_iter_size)\n",
        "        fixed_noise = sample_noise(batch_size).to(\n",
        "            device\n",
        "        )  # used to save samples every few epochs\n",
        "\n",
        "        gan_model_name = \"gan_{}.tar\".format(model_prefix)\n",
        "\n",
        "        first_iter = 0\n",
        "        if take_backup and os.path.isfile(gan_model_name):\n",
        "            if cuda:\n",
        "                checkpoint = torch.load(gan_model_name)\n",
        "            else:\n",
        "                checkpoint = torch.load(gan_model_name, map_location=\"cpu\")\n",
        "            self.generator.load_state_dict(checkpoint[\"generator\"])\n",
        "            self.discriminator.load_state_dict(checkpoint[\"discriminator\"])\n",
        "            self.optimizer_d.load_state_dict(checkpoint[\"optimizer_d\"])\n",
        "            self.optimizer_g.load_state_dict(checkpoint[\"optimizer_g\"])\n",
        "            self.train_d_cost = checkpoint[\"train_d_cost\"]\n",
        "            self.train_w_distance = checkpoint[\"train_w_distance\"]\n",
        "            self.valid_g_cost = checkpoint[\"valid_g_cost\"]\n",
        "            self.g_cost = checkpoint[\"g_cost\"]\n",
        "\n",
        "            first_iter = checkpoint[\"n_iterations\"]\n",
        "            for i in range(0, first_iter, progress_bar_step_iter_size):\n",
        "                progress_bar.update()\n",
        "            self.generator.eval()\n",
        "            with torch.no_grad():\n",
        "                fake = self.generator(fixed_noise).detach().cpu().numpy()\n",
        "            save_samples(fake, first_iter)\n",
        "        self.generator.train()\n",
        "        self.discriminator.train()\n",
        "        for iter_indx in range(first_iter, n_iterations):\n",
        "            self.enable_disc_disable_gen()\n",
        "            for _ in range(n_critic):\n",
        "                real_signal = next(self.train_loader)\n",
        "\n",
        "                # need to add mixed signal and flag\n",
        "                noise = sample_noise(batch_size * generator_batch_size_factor)\n",
        "                generated = self.generator(noise)\n",
        "                #############################\n",
        "                # Calculating discriminator loss and updating discriminator\n",
        "                #############################\n",
        "                self.apply_zero_grad()\n",
        "                disc_cost, disc_wd = self.calculate_discriminator_loss(\n",
        "                    real_signal.data, generated.data\n",
        "                )\n",
        "                assert not (torch.isnan(disc_cost))\n",
        "                disc_cost.backward()\n",
        "                self.optimizer_d.step()\n",
        "\n",
        "            if self.validate and iter_indx % store_cost_every == 0:\n",
        "                self.disable_all()\n",
        "                val_data = next(self.val_loader)\n",
        "                val_real = val_data\n",
        "                with torch.no_grad():\n",
        "                    val_discriminator_output = self.discriminator(val_real)\n",
        "                    val_generator_cost = val_discriminator_output.mean()\n",
        "                    self.valid_g_cost.append(val_generator_cost.item())\n",
        "\n",
        "            #############################\n",
        "            # (2) Update G network every n_critic steps\n",
        "            #############################\n",
        "            self.apply_zero_grad()\n",
        "            self.enable_gen_disable_disc()\n",
        "            noise = sample_noise(batch_size * generator_batch_size_factor)\n",
        "            generated = self.generator(noise)\n",
        "            discriminator_output_fake = self.discriminator(generated)\n",
        "            generator_cost = -discriminator_output_fake.mean()\n",
        "            generator_cost.backward()\n",
        "            self.optimizer_g.step()\n",
        "            self.disable_all()\n",
        "\n",
        "            if iter_indx % store_cost_every == 0:\n",
        "                self.g_cost.append(generator_cost.item() * -1)\n",
        "                self.train_d_cost.append(disc_cost.item())\n",
        "                self.train_w_distance.append(disc_wd.item() * -1)\n",
        "\n",
        "                progress_updates = {\n",
        "                    \"Loss_D WD\": str(self.train_w_distance[-1]),\n",
        "                    \"Loss_G\": str(self.g_cost[-1]),\n",
        "                    \"Val_G\": str(self.valid_g_cost[-1]),\n",
        "                }\n",
        "                progress_bar.set_postfix(progress_updates)\n",
        "\n",
        "            if iter_indx % progress_bar_step_iter_size == 0:\n",
        "                progress_bar.update()\n",
        "            # lr decay\n",
        "            if decay_lr:\n",
        "                decay = max(0.0, 1.0 - (iter_indx * 1.0 / n_iterations))\n",
        "                # update the learning rate\n",
        "                update_optimizer_lr(self.optimizer_d, lr_d, decay)\n",
        "                update_optimizer_lr(self.optimizer_g, lr_g, decay)\n",
        "\n",
        "            if iter_indx % save_samples_every == 0:\n",
        "                with torch.no_grad():\n",
        "                    latent_space_interpolation(self.generator, n_samples=2)\n",
        "                    fake = self.generator(fixed_noise).detach().cpu().numpy()\n",
        "                save_samples(fake, iter_indx)\n",
        "\n",
        "            if take_backup and iter_indx % backup_every_n_iters == 0:\n",
        "                saving_dict = {\n",
        "                    \"generator\": self.generator.state_dict(),\n",
        "                    \"discriminator\": self.discriminator.state_dict(),\n",
        "                    \"n_iterations\": iter_indx,\n",
        "                    \"optimizer_d\": self.optimizer_d.state_dict(),\n",
        "                    \"optimizer_g\": self.optimizer_g.state_dict(),\n",
        "                    \"train_d_cost\": self.train_d_cost,\n",
        "                    \"train_w_distance\": self.train_w_distance,\n",
        "                    \"valid_g_cost\": self.valid_g_cost,\n",
        "                    \"g_cost\": self.g_cost,\n",
        "                }\n",
        "                torch.save(saving_dict, gan_model_name)\n",
        "\n",
        "        self.generator.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Sm5LQOMxtS"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4SFWACMMARi"
      },
      "outputs": [],
      "source": [
        "train_loader = WavDataLoader(os.path.join(target_signals_dir, \"training\"))\n",
        "val_loader = WavDataLoader(os.path.join(target_signals_dir, \"validation\"))\n",
        "\n",
        "wave_gan = WaveGan_GP(train_loader, val_loader)\n",
        "wave_gan.train()\n",
        "visualize_loss(\n",
        "    wave_gan.g_cost, wave_gan.valid_g_cost, \"Train\", \"Val\", \"Negative Critic Loss\"\n",
        ")\n",
        "latent_space_interpolation(wave_gan.generator, n_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WecYCIZfHI0e"
      },
      "outputs": [],
      "source": [
        "!cp -r output /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r visualization /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "5ja6wKohj1Z8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}